{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from datetime import date, time, datetime\n",
    "from datetime import timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.utils import np_utils\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix,  accuracy_score\n",
    "\n",
    "\n",
    "def data_select(fn_weather,fn_price):\n",
    "    r_data = pd.read_csv(fn_weather, encoding=\"utf-8\")\n",
    "    data_t_n = ['Temperature', 'RH', 'StnPres', 'WS', 'WSGust','d_tmp']\n",
    "    col_fix =  ['T.Max', 'T.Min', 'Precp','Temperature', 'RH', 'StnPres', 'WS', 'WSGust']\n",
    "    \n",
    "    # rm \"/\"\n",
    "    for col in col_fix:\n",
    "        old = r_data[col]\n",
    "        new = []\n",
    "        for i in range(len(old)):\n",
    "            try:\n",
    "                new.append(float(old[i]))\n",
    "                tmp = float(old[i])\n",
    "            except ValueError:\n",
    "                new.append(tmp)\n",
    "        r_data[col] = new\n",
    "    rain  = r_data['Precp']\n",
    "    \n",
    "    # rain_class function\n",
    "    def rain_c(precp):\n",
    "        rain_class=[0]\n",
    "        rain_tag = [\"no_rain\", \"rain\"]\n",
    "        result = 0\n",
    "        for d in rain_class:\n",
    "            if precp > d:\n",
    "                result = result + 1\n",
    "        return result\n",
    "\n",
    "    rain_class=[]\n",
    "    for precp in rain:\n",
    "        rain_class.append(rain_c(precp))\n",
    "    \n",
    "    d_tmp = r_data['T.Max'] - r_data['T.Min']\n",
    "    r_data[\"d_tmp\"]= d_tmp\n",
    "    d_data = r_data[\"Date\"]\n",
    "    drop_c =[\"Date\",'T.Max', 'T.Min', 'Precp']\n",
    "    r_data = r_data.drop(columns=drop_c)\n",
    "    r_data[\"rain\"]= rain_class\n",
    "#     r_data.describe()\n",
    "\n",
    "    def to_zscore(df, col_x):\n",
    "    #     df_tmp = df\n",
    "    #     col = \"Temperature\"\n",
    "        for col in col_x:\n",
    "            df[col] = stats.zscore(df[col])\n",
    "            df[col] = np.round(df[col], decimals=4)\n",
    "    # to_zscore(r_data, data_t_n)\n",
    "\n",
    "    def to_zscore2(df):\n",
    "        col_x = df.columns.to_list()\n",
    "        mean = df.mean(axis=0)\n",
    "        std = df.std(axis=0)\n",
    "        for i in range(len(col_x)):\n",
    "            df[col_x[i]]=(df[col_x[i]]-mean[i])/std[i]\n",
    "        return  df , (mean, std)\n",
    "    r_data, recordz = to_zscore2(r_data)\n",
    "    \n",
    "    return r_data, recordz , d_data\n",
    "\n",
    "def data_shift(r_data, d_data, shift1, shift2=0):\n",
    "    # shift and conbinde Data~ D-28 (D0~D-28)\n",
    "    dfs =[]\n",
    "    \n",
    "    shifts = shift1\n",
    "    \n",
    "    if shift2==0:\n",
    "        df_s = r_data.copy()\n",
    "    else:\n",
    "        df_s = r_data.copy()\n",
    "        df_s = df_s.shift(periods=shift2)\n",
    "        d_data = d_data[:(-shift2)]\n",
    "        \n",
    "#     add shift base\n",
    "    arr_all = np.array(df_s)\n",
    "    \n",
    "    for i in range(1,shifts+1):\n",
    "        tp = np.array(df_s.shift(periods=i))\n",
    "        arr_all = np.concatenate((arr_all, tp), axis=1)\n",
    "    \n",
    "    df_all = pd.DataFrame(arr_all)\n",
    "    \n",
    "    df_all = df_all.dropna()\n",
    "    df_all = df_all.reset_index()\n",
    "    df_all = df_all.drop(columns=\"index\")\n",
    "    d_data = d_data[:(-shifts)]\n",
    "    \n",
    "    r_data = df_all\n",
    "\n",
    "    d_date = d_data[0]\n",
    "    \n",
    "    def trans_to_y_w(d_date):\n",
    "        year = int(d_date.split(\"-\")[0])\n",
    "        d_day = date(year,int(d_date.split(\"-\")[1]), int(d_date.split(\"-\")[2]))- date(year, 1, 1)\n",
    "        d_w = 1+ (d_day.days // 7)\n",
    "        if d_w ==53:\n",
    "            d_w = 52\n",
    "        result =  str(year) + \"_\" + str(d_w)\n",
    "\n",
    "        if d_day.days % 7 ==6:\n",
    "            return result\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    d_new = []\n",
    "    for i in range(len(d_data)):\n",
    "        d_new.append(trans_to_y_w(d_data[i]))\n",
    "    r_data[\"y_w\"] = d_new\n",
    "    r_data = r_data.dropna()\n",
    "    \n",
    "    df_pbw = pd.read_csv(fn_price, encoding=\"utf-8\")\n",
    "    \n",
    "    drop_c = [\"year\", \"week\"]\n",
    "    df_pbw = df_pbw.drop(columns=drop_c)\n",
    "\n",
    "    df_join = df_pbw.join(r_data.set_index(\"y_w\"), on=\"y_w\")\n",
    "    df_join = df_join.dropna()\n",
    "    \n",
    "    d_t_dummy = [\"market\"]\n",
    "    date_info = pd.DataFrame()\n",
    "    for col in d_t_dummy:\n",
    "        date_info[col] = df_join[col]\n",
    "        dummy = pd.get_dummies(date_info[col])\n",
    "        df_join = pd.concat([df_join, dummy], axis=1)\n",
    "    \n",
    "    drop_c = [\"market\", \"w_avg\", \"w_sale\", \"y_w\",\"price_diff\"]\n",
    "    y_raw = np.array(df_join[\"price_diff\"])\n",
    "#     print(\"y_price_diff describe:\")\n",
    "#     print(df_join[\"price_diff\"].describe())\n",
    "#     print(\"======\")\n",
    "    df_join = df_join.drop(columns=drop_c)\n",
    "    \n",
    "    def y_to_class(v):\n",
    "        t = 0\n",
    "        y_class_range = []\n",
    "        for q in range(1,12):\n",
    "            y_class_range.append(round((q*0.1-0.6),4))\n",
    "\n",
    "        for i in range(len(y_class_range)):\n",
    "            if (v >= y_class_range[i]):\n",
    "                t = i+1\n",
    "        return int(t)\n",
    "\n",
    "    y_class = []\n",
    "\n",
    "    for i in range(len(y_raw)):\n",
    "        y_class.append(y_to_class(y_raw[i]))\n",
    "#     print(\"freq:\",np.bincount(y_class))\n",
    "    \n",
    "    \n",
    "    \n",
    "    x = np.array(df_join)\n",
    "    y = y_class\n",
    "    input_d = x.shape[1]\n",
    "    \n",
    "    \n",
    "    return x, y, input_d\n",
    "\n",
    "def model_run(x, y, input_d, shifts, epochs=40):\n",
    "    \n",
    "    out_class_count = len(np.bincount(y))\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n",
    "    model = Sequential()\n",
    "    layer_0 = Dense(units =input_d//4, input_dim = input_d,\n",
    "                    kernel_initializer = \"random_normal\", activation = \"relu\")\n",
    "    model.add(layer_0)\n",
    "    model.add(Dropout(0.25))\n",
    "    layer_1 = Dense(units =shifts,kernel_initializer = \"random_normal\", activation = \"relu\")\n",
    "    model.add(layer_1)\n",
    "    model.add(Dropout(0.25))\n",
    "    layer_out = Dense(units = out_class_count,kernel_initializer = \"random_normal\", \n",
    "                      activation = \"softmax\")\n",
    "    model.add(layer_out)\n",
    "#     model.summary()\n",
    "    model.compile(loss=\"categorical_crossentropy\",optimizer = \"adam\", metrics = ['accuracy'])\n",
    "    train_history = model.fit(x = x_train, y = np_utils.to_categorical(y_train), \n",
    "                              validation_split = 0.1, epochs =epochs, verbose = 0)\n",
    "    \n",
    "#     plt.plot(train_history.history[\"loss\"])\n",
    "#     plt.plot(train_history.history[\"val_loss\"])\n",
    "#     plt.title(\"Loss Graph\")\n",
    "#     plt.legend(['loss', 'val_loss'], loc=\"upper left\")\n",
    "    pre = model.predict_classes(x_test)\n",
    "    acc = round(accuracy_score(y_test, pre)*100,2)\n",
    "#     print(\"acc:\", round(accuracy_score(y_test, pre)*100,2),\"%\")\n",
    "\n",
    "    return acc, pd.DataFrame(confusion_matrix(y_test, pre)), model\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_list= glob.glob(\"../result/merge_weather/*.csv\")\n",
    "# pd.set_option('display.max_rows', 20)\n",
    "# w_list\n",
    "# fn_weather = w_list[0]\n",
    "# fn_price = \"../result/flower_price_byweek/Rose_pbyweek.csv\"\n",
    "\n",
    "\n",
    "# shift1 = 10\n",
    "\n",
    "# def test_base(fn_weather, fn_price, shift1):\n",
    "#     r_data, recordz, d_data = data_select(fn_weather,fn_price)\n",
    "#     plx = []\n",
    "#     ply = []\n",
    "#     for shft2 in range(7,42+1,7):\n",
    "#         x, y, input_d = data_shift(r_data, d_data, shift1, shift2=0)\n",
    "#         acc, df_conf, model = model_run(x, y, input_d, shift1, epochs=40)\n",
    "#         print(shft2,acc)\n",
    "#         plx.append(shft2)\n",
    "#         ply.append(acc)\n",
    "\n",
    "#     plt.plot(plx, ply)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # 玫瑰 : 台中，彰化，雲林，嘉義，南投，高雄，屏東\n",
    "# 洋桔梗 : 南投，彰化，雲林，嘉義，台南\n",
    "# 大菊 : 彰化，雲林，南投\n",
    "# 火鶴花 : 台中，嘉義，南投，台南，高雄，屏東\n",
    "# 香水百合 :台中，南投，嘉義\n",
    "\n",
    "# flowers = []\n",
    "# for fn in get_f_list:\n",
    "#     flowers.append(fn.split(\"\\\\\")[-1].split(\"_\")[0])\n",
    "\n",
    "# flower = flowers[0]\n",
    "# fn_list = glob.glob(\"../result/merge_flower_price/*\" + flower + \"*.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# w_list= glob.glob(\"../result/merge_weather/*.csv\")\n",
    "# p_list = glob.glob(\"../result/flower_price_byweek/*\")\n",
    "# pd.set_option('display.max_rows', 20)\n",
    "# # w_list\n",
    "# # fn_weather = w_list[0]\n",
    "# # fn_price = p_list[0]\n",
    "# def test__shift1_allstations(w_list, p_list, shiftt2=0):\n",
    "#     result_dataset =[]\n",
    "\n",
    "#     for fn_weather in w_list:\n",
    "#         print(fn_weather)\n",
    "#         for fn_price in p_list:\n",
    "#             print(fn_price)\n",
    "#             rs_data = []\n",
    "#             test_w = fn_weather.split(\"\\\\\")[-1].split(\".\")[0] \n",
    "#             test_p = fn_price.split(\"\\\\\")[-1].split(\"_\")[0]\n",
    "#             rs_data.append(test_w)\n",
    "#             rs_data.append(test_p)\n",
    "#             r_data, recordz, d_data = data_select(fn_weather,fn_price)\n",
    "#             tmp_acc =[]\n",
    "#             for shift1 in range(7,20,3):\n",
    "#                 x, y, input_d = data_shift(r_data, d_data, shift1, shift2=7)\n",
    "#                 acc, df_conf, model = model_run(x, y, input_d, shift1, epochs=40)\n",
    "#                 tmp_acc.append(acc)\n",
    "#             tmp_acc = np.array(tmp_acc)\n",
    "#     #         rs_data.append(tmp_acc.mean())\n",
    "#             rs_data.append(tmp_acc.max())\n",
    "#             rs_data.append(tmp_acc.min())\n",
    "#             result_dataset.append(rs_data)\n",
    "\n",
    "#     df = pd.DataFrame(result_dataset,columns=[\"station_flower\", \"mean\", \"max\", \"min\"])\n",
    "#     df.to_csv(\"test_0517_shift2_7.csv\", encoding=\"utf-8\")\n",
    "        \n",
    "# # shift1 = 10\n",
    "# # r_data, recordz, d_data = data_select(fn_weather,fn_price)\n",
    "# # plx = []\n",
    "# # ply = []\n",
    "# # for shft2 in range(7,28+1,7):\n",
    "# #     x, y, input_d = data_shift(r_data, d_data, shift1, shift2=0)\n",
    "# #     acc, result = model_run(x, y, input_d, shift1, epochs=40)\n",
    "# #     print(shft2,acc)\n",
    "# #     plx.append(shft2)\n",
    "# #     ply.append(acc)\n",
    "\n",
    "# # plt.plot(plx, ply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C0F9L0_后里', 'Anthurium']\n",
      "shift= 5\n",
      "epochs =  30 acc =  37.52\n",
      "epochs =  60 acc =  39.65\n",
      "epochs =  80 acc =  40.28\n",
      "shift= 7\n",
      "epochs =  30 acc =  43.54\n",
      "epochs =  40 acc =  49.06\n"
     ]
    }
   ],
   "source": [
    "w_list= glob.glob(\"../result/merge_weather/*.csv\")\n",
    "p_list = glob.glob(\"../result/flower_price_byweek/*\")\n",
    "select_t = [(1,0),(3,2),(2,1),(2,3),(2,4)]\n",
    "\n",
    "\n",
    "def run_model_build(fn_weather,fn_price,shift2=0):\n",
    "    rs_data = []\n",
    "    test_w = fn_weather.split(\"\\\\\")[-1].split(\".\")[0] \n",
    "    test_p = fn_price.split(\"\\\\\")[-1].split(\"_\")[0]\n",
    "    rs_data.append(test_w)\n",
    "    rs_data.append(test_p)\n",
    "    print(rs_data)\n",
    "    r_data, recordz, d_data = data_select(fn_weather,fn_price)\n",
    "    rs_data.append(recordz)\n",
    "    tmp_acc =[]\n",
    "    acc_cf = 0\n",
    "    for shift1 in range(5,14+1,2):\n",
    "        print(\"shift=\", shift1)\n",
    "        for epochs in range(30,80+1,10):\n",
    "            x, y, input_d = data_shift(r_data, d_data, shift1, shift2)\n",
    "            acc, df_conf, model = model_run(x, y, input_d, shift1, epochs)\n",
    "            tmp_acc.append(acc)\n",
    "            if acc > acc_cf:\n",
    "                acc_cf = acc\n",
    "                best_model = model\n",
    "                set_info = (shift1, shift2, epochs)\n",
    "                print(\"epochs = \", epochs,\"acc = \",acc)\n",
    "    rs_data.append([acc_cf, set_info])        \n",
    "    rs_data.append(best_model)\n",
    "    return rs_data\n",
    "\n",
    "\n",
    "# for st, flower in select_t:\n",
    "st, flower = select_t[0]\n",
    "fn_weather = w_list[st]\n",
    "fn_price = p_list[flower]\n",
    "result = run_model_build(fn_weather,fn_price)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_list= glob.glob(\"../result/merge_weather/*.csv\")\n",
    "# p_list = glob.glob(\"../result/flower_price_byweek/*\")\n",
    "# pd.set_option('display.max_rows', 20)\n",
    "# w_list\n",
    "# wc = [0,1,4,-1]\n",
    "# w_list = list(np.array(w_list)[wc])\n",
    "\n",
    "# result_dataset =[]\n",
    "# def run_model_build(w_list,p_list):\n",
    "#     rs_data = []\n",
    "#     test_w = fn_weather.split(\"\\\\\")[-1].split(\".\")[0] \n",
    "#     test_p = fn_price.split(\"\\\\\")[-1].split(\"_\")[0]\n",
    "#     rs_data.append(test_w)\n",
    "#     rs_data.append(test_p)\n",
    "#     r_data, recordz, d_data = data_select(fn_weather,fn_price)\n",
    "#     tmp_acc =[]\n",
    "#     for shift1 in range(7,20,4):\n",
    "#         x, y, input_d = data_shift(r_data, d_data, shift1, shift2=14)\n",
    "#         acc, df_conf, model = model_run(x, y, input_d, shift1, epochs=40)\n",
    "#         tmp_acc.append(acc)\n",
    "#     tmp_acc = np.array(tmp_acc)\n",
    "# #         rs_data.append(tmp_acc.mean())\n",
    "#     rs_data.append(tmp_acc.max())\n",
    "#     rs_data.append(tmp_acc.min())\n",
    "#     result_dataset.append(rs_data)\n",
    "#     return result_dataset\n",
    "\n",
    "\n",
    "# df = pd.DataFrame(result_dataset,columns=[\"station\", \"flower\", \"max\", \"min\"])\n",
    "# df.to_csv(\"test_0517_shift2_14.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
