{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../result/merge_weather\\C0F9L0_后里.csv ../result/flower_price_byweek\\Anthurium_pbyweek.csv\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from datetime import date, time, datetime\n",
    "from datetime import timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.utils import np_utils\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix,  accuracy_score\n",
    "\n",
    "w_list= glob.glob(\"../result/merge_weather/*.csv\")\n",
    "p_list = glob.glob(\"../result/flower_price_byweek/*\")\n",
    "select_t = [(1,0),(3,2),(2,1),(2,3),(2,4)]\n",
    "st, flower = select_t[0]\n",
    "fn_weather = w_list[st]\n",
    "fn_price = p_list[flower]\n",
    "\n",
    "print(fn_weather, fn_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_data = pd.read_csv(fn_weather, encoding=\"utf-8\")\n",
    "lc_d = pd.read_csv(\"../lunar_celeb_date.csv\", encoding=\"utf-8\")\n",
    "col_fix =  ['T.Max', 'T.Min', 'Precp','Temperature', 'RH', 'StnPres', 'WS', 'WSGust']\n",
    "r_data = r_data.join(lc_d.set_index(\"date\"), on=\"Date\")\n",
    "# rm \"/\"\n",
    "for col in col_fix:\n",
    "    old = r_data[col]\n",
    "    new = []\n",
    "    for i in range(len(old)):\n",
    "        try:\n",
    "            new.append(float(old[i]))\n",
    "            tmp = float(old[i])\n",
    "        except ValueError:\n",
    "            new.append(tmp)\n",
    "    r_data[col] = new\n",
    "d_tmp = r_data['T.Max'] - r_data['T.Min']\n",
    "r_data[\"d_tmp\"]= d_tmp\n",
    "\n",
    "# extract_date\n",
    "d_data = r_data[\"Date\"]\n",
    "drop_c =[\"Date\",'T.Max', 'T.Min']\n",
    "r_data = r_data.drop(columns=drop_c)\n",
    "\n",
    "def to_zscore2(df):\n",
    "    col_x = df.columns.to_list()\n",
    "    mean = df.mean(axis=0)\n",
    "    std = df.std(axis=0)\n",
    "    for i in range(len(col_x)):\n",
    "        df[col_x[i]]=(df[col_x[i]]-mean[i])/std[i]\n",
    "    return  df , (mean, std)\n",
    "\n",
    "r_data, recordz = to_zscore2(r_data)\n",
    "\n",
    "# r_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(r_data.shape)\n",
    "# print(d_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift1=28\n",
    "shift2=1\n",
    "\n",
    "dfs =[]\n",
    "arr_all =[]\n",
    "if shift2==0:\n",
    "    df_s = r_data.copy()\n",
    "else:\n",
    "    df_s = r_data.copy()\n",
    "    df_s = df_s.shift(periods=shift2)\n",
    "#     d_data = np.array(pd.DataFrame(d_data).shift(periods=shift2))\n",
    "#     add shift base\n",
    "arr_all = np.array(df_s)\n",
    "\n",
    "for i in range(1,shift1):\n",
    "    tp = np.array(df_s.shift(periods=i))\n",
    "    arr_all = np.concatenate((arr_all, tp), axis=1)\n",
    "df_all = pd.DataFrame(arr_all)\n",
    "df_all[\"date\"] = d_data\n",
    "df_all = df_all.dropna()\n",
    "df_all = df_all.reset_index()\n",
    "df_all = df_all.drop(columns=\"index\")\n",
    "\n",
    "# r_data = df_all\n",
    "\n",
    "# df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_data = df_all\n",
    "\n",
    "d_data = r_data[\"date\"]\n",
    "\n",
    "def trans_to_y_w(d_date):\n",
    "    year = int(d_date.split(\"-\")[0])\n",
    "    d_day = date(year,int(d_date.split(\"-\")[1]), int(d_date.split(\"-\")[2]))- date(year, 1, 1)\n",
    "    d_w = 1+ (d_day.days // 7)\n",
    "    if d_w ==53:\n",
    "        d_w = 52\n",
    "    result =  str(year) + \"_\" + str(d_w)\n",
    "\n",
    "    if d_day.days % 7 ==6:\n",
    "        return result\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "d_new = []\n",
    "for i in range(len(d_data)):\n",
    "    d_new.append(trans_to_y_w(d_data[i]))\n",
    "r_data[\"y_w\"] = d_new\n",
    "df_all = df_all.dropna()\n",
    "df_all = df_all.reset_index()\n",
    "df_all = df_all.drop(columns=\"index\")\n",
    "# r_data\n",
    "# d_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pbw = pd.read_csv(fn_price, encoding=\"utf-8\")\n",
    "# join x and y\n",
    "df_join = df_pbw.join(r_data.set_index(\"y_w\"), on=\"y_w\")\n",
    "df_join = df_join.dropna()\n",
    "df_join = df_join.reset_index()\n",
    "df_join = df_join.drop(columns=\"index\")\n",
    "# extract y info\n",
    "y_date = df_join[\"date\"]\n",
    "y_yw = df_join[\"y_w\"]\n",
    "y_raw = np.array(df_join[\"price_diff\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7874, 621)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "d_t_dummy = [\"market\"]\n",
    "tmp_dum = pd.DataFrame()\n",
    "for col in d_t_dummy:\n",
    "    tmp_dum[col] = df_join[col]\n",
    "    dummy = pd.get_dummies(tmp_dum[col])\n",
    "    df_join = pd.concat([df_join, dummy], axis=1)\n",
    "drop_c = [\"market\",\"year\", \"week\",\"w_avg\", \"w_sale\", \"date\", \"y_w\", \"price_diff\"]\n",
    "# x finished\n",
    "x = np.array(df_join.drop(columns=drop_c))\n",
    "# x\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_price_diff describe:\n",
      "       y_price_diff\n",
      "count   7874.000000\n",
      "mean       0.009691\n",
      "std        0.155143\n",
      "min       -0.410906\n",
      "25%       -0.092490\n",
      "50%       -0.005299\n",
      "75%        0.093753\n",
      "max        0.791508\n",
      "======\n",
      "freq: [ 511 1285 2236 1966  979  897]\n",
      "(7874, 621) 7874\n"
     ]
    }
   ],
   "source": [
    "# modify y\n",
    "# y_raw = np.array(df_join[\"price_diff\"])\n",
    "print(\"y_price_diff describe:\")\n",
    "print(pd.DataFrame(y_raw, columns=[\"y_price_diff\"]).describe())\n",
    "print(\"======\")\n",
    "def y_to_class(v):\n",
    "    t = 0\n",
    "    y_class_range = [-0.2,-0.1,0,0.1,0.2]\n",
    "#     for q in range(1,12):\n",
    "#         y_class_range.append(round((q*0.1-0.6),4))\n",
    "\n",
    "    for i in range(len(y_class_range)):\n",
    "        if (v >= y_class_range[i]):\n",
    "            t = i+1\n",
    "    return int(t)\n",
    "\n",
    "y_class = []\n",
    "\n",
    "for i in range(len(y_raw)):\n",
    "    y_class.append(y_to_class(y_raw[i]))\n",
    "print(\"freq:\",np.bincount(y_class))\n",
    "\n",
    "y = np.array(y_class)\n",
    "input_d = x.shape[1]\n",
    "print(x.shape,len(y))\n",
    "# x y ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2014 2015 2016 2017 2018]\n",
      "[1488 1612 1581 1612 1581]\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "# y_date\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n",
    "y_year = []\n",
    "for i in range(len(y_yw)):\n",
    "    y_year.append(int(y_yw[i].split(\"_\")[0]))\n",
    "y_year = np.array(y_year)\n",
    "years, count = np.unique(np.array(y_year), return_counts=True)\n",
    "print(years)\n",
    "print(count)\n",
    "\n",
    "train = y_year[:]<=2016\n",
    "test = y_year[:]>2016\n",
    "\n",
    "x_train, y_train = x[train,:], y[train]\n",
    "x_test, y_test = x[test,:], y[test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_class_count = len(np.bincount(y))\n",
    "# shifts = 28\n",
    "# xy_data = (x_train, y_train, x_test, y_test)\n",
    "# def model_build(xy_data, input_d, shifts, epochs=60):\n",
    "#     x_train, y_train, x_test, y_test = xy_data\n",
    "    \n",
    "#     model = Sequential()\n",
    "#     layer_0 = Dense(units =input_d//4, input_dim = input_d,\n",
    "#                     kernel_initializer = \"random_normal\", activation = \"relu\")\n",
    "#     model.add(layer_0)\n",
    "#     model.add(Dropout(0.25))\n",
    "#     layer_1 = Dense(units =shifts,kernel_initializer = \"random_normal\", activation = \"relu\")\n",
    "#     model.add(layer_1)\n",
    "#     model.add(Dropout(0.25))\n",
    "#     layer_out = Dense(units = out_class_count,kernel_initializer = \"random_normal\", \n",
    "#                       activation = \"softmax\")\n",
    "#     model.add(layer_out)\n",
    "#     #     model.summary()\n",
    "#     model.compile(loss=\"categorical_crossentropy\",optimizer = \"adam\", metrics = ['accuracy'])\n",
    "#     train_history = model.fit(x = x_train, y = np_utils.to_categorical(y_train), \n",
    "#                               validation_split = 0.1, epochs =epochs, verbose = 2)\n",
    "    \n",
    "#     #     model_test\n",
    "#     pre = model.predict_classes(x_test)\n",
    "#     acc = round(accuracy_score(y_test, pre)*100,2)\n",
    "    \n",
    "#     return acc, model, train_history\n",
    "\n",
    "# acc_cf = 0\n",
    "\n",
    "# for epochs in range(5,80,5):\n",
    "#     print(\"epochs:\", epochs)\n",
    "#     acc, model ,train_history= model_build(xy_data, input_d, shifts, epochs=60)\n",
    "#     print(train_history)\n",
    "#     print(acc)\n",
    "#     if acc > acc_cf:\n",
    "#         acc_cf = acc\n",
    "#         bestmodel = model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_class_count = len(np.bincount(y))\n",
    "# # shifts = 28\n",
    "# result_table = []\n",
    "# max_res = []\n",
    "# acc_cf =0\n",
    "# dinp, epochs = (6,6)\n",
    "# for i in range(10):\n",
    "#     #     epochs = 80\n",
    "#     # xy_data = (x_train, y_train, x_test, y_test)\n",
    "#     # x_train, y_train, x_test, y_test = xy_data\n",
    "#     model = Sequential()\n",
    "#     layer_0 = Dense(units =input_d//dinp, input_dim = input_d,\n",
    "#                     kernel_initializer = \"random_normal\", activation = \"relu\")\n",
    "#     model.add(layer_0)\n",
    "#     model.add(Dropout(0.25))\n",
    "#     # layer_1 = Dense(units =shift1,kernel_initializer = \"random_normal\", activation = \"relu\")\n",
    "#     # model.add(layer_1)\n",
    "#     # model.add(Dropout(0.25))\n",
    "#     layer_out = Dense(units = out_class_count,kernel_initializer = \"random_normal\", \n",
    "#                       activation = \"softmax\")\n",
    "#     model.add(layer_out)\n",
    "#     #     model.summary()\n",
    "#     model.compile(loss=\"categorical_crossentropy\",optimizer = \"adam\", metrics = ['accuracy'])\n",
    "#     train_history = model.fit(x = x_train, y = np_utils.to_categorical(y_train), \n",
    "#                               validation_split = 0.2, epochs =epochs, verbose = 0)\n",
    "#     #     plt.plot(train_history.history[\"loss\"])\n",
    "#     #     plt.plot(train_history.history[\"val_loss\"])\n",
    "#     #     plt.title(\"Loss Graph\")\n",
    "#     #     plt.legend(['loss', 'val_loss'], loc=\"upper left\")\n",
    "\n",
    "#     #     model_test\n",
    "#     pre = model.predict_classes(x_test)\n",
    "#     acc = round(accuracy_score(y_test, pre)*100,2)\n",
    "#     #         if acc > acc_cf:\n",
    "#     #             max_res = dinp, epochs, acc\n",
    "#     #             acc_cf = acc\n",
    "#     #         result_table.append([dinp, epochs, acc])\n",
    "#     print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc\n",
    "# pd.DataFrame(result_table)\n",
    "# max_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# pre = model.predict_classes(x_test)\n",
    "# pd.DataFrame(confusion_matrix(y_test, pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3744 samples, validate on 937 samples\n",
      "Epoch 1/50\n",
      " - 0s - loss: 1.4378 - val_loss: 0.6540\n",
      "Epoch 2/50\n",
      " - 0s - loss: 0.6671 - val_loss: 0.6756\n",
      "Epoch 3/50\n",
      " - 0s - loss: 0.6363 - val_loss: 0.6471\n",
      "Epoch 4/50\n",
      " - 0s - loss: 0.6158 - val_loss: 0.6389\n",
      "Epoch 5/50\n",
      " - 0s - loss: 0.5684 - val_loss: 0.6474\n",
      "Epoch 6/50\n",
      " - 0s - loss: 0.5497 - val_loss: 0.6315\n",
      "Epoch 7/50\n",
      " - 0s - loss: 0.5666 - val_loss: 0.6557\n",
      "Epoch 8/50\n",
      " - 0s - loss: 0.5573 - val_loss: 0.6084\n",
      "Epoch 9/50\n",
      " - 0s - loss: 0.5494 - val_loss: 0.6172\n",
      "Epoch 10/50\n",
      " - 0s - loss: 0.5346 - val_loss: 0.5987\n",
      "Epoch 11/50\n",
      " - 0s - loss: 0.5367 - val_loss: 0.6077\n",
      "Epoch 12/50\n",
      " - 0s - loss: 0.5033 - val_loss: 0.5956\n",
      "Epoch 13/50\n",
      " - 0s - loss: 0.5137 - val_loss: 0.5887\n",
      "Epoch 14/50\n",
      " - 0s - loss: 0.4946 - val_loss: 0.6079\n",
      "Epoch 15/50\n",
      " - 0s - loss: 0.4759 - val_loss: 0.5594\n",
      "Epoch 16/50\n",
      " - 0s - loss: 0.4571 - val_loss: 0.5654\n",
      "Epoch 17/50\n",
      " - 0s - loss: 0.4319 - val_loss: 0.5710\n",
      "Epoch 18/50\n",
      " - 0s - loss: 0.4216 - val_loss: 0.5173\n",
      "Epoch 19/50\n",
      " - 0s - loss: 0.3791 - val_loss: 0.4937\n",
      "Epoch 20/50\n",
      " - 0s - loss: 0.3741 - val_loss: 0.4894\n",
      "Epoch 21/50\n",
      " - 0s - loss: 0.3548 - val_loss: 0.4649\n",
      "Epoch 22/50\n",
      " - 0s - loss: 0.3463 - val_loss: 0.4789\n",
      "Epoch 23/50\n",
      " - 0s - loss: 0.3238 - val_loss: 0.4563\n",
      "Epoch 24/50\n",
      " - 0s - loss: 0.3163 - val_loss: 0.4462\n",
      "Epoch 25/50\n",
      " - 0s - loss: 0.2982 - val_loss: 0.4447\n",
      "Epoch 26/50\n",
      " - 0s - loss: 0.2907 - val_loss: 0.4214\n",
      "Epoch 27/50\n",
      " - 0s - loss: 0.2850 - val_loss: 0.4441\n",
      "Epoch 28/50\n",
      " - 0s - loss: 0.2810 - val_loss: 0.4162\n",
      "Epoch 29/50\n",
      " - 0s - loss: 0.2799 - val_loss: 0.4230\n",
      "Epoch 30/50\n",
      " - 0s - loss: 0.2768 - val_loss: 0.4267\n",
      "Epoch 31/50\n",
      " - 0s - loss: 0.2628 - val_loss: 0.4057\n",
      "Epoch 32/50\n",
      " - 0s - loss: 0.2687 - val_loss: 0.3910\n",
      "Epoch 33/50\n",
      " - 0s - loss: 0.2541 - val_loss: 0.3847\n",
      "Epoch 34/50\n",
      " - 0s - loss: 0.2432 - val_loss: 0.3902\n",
      "Epoch 35/50\n",
      " - 0s - loss: 0.2499 - val_loss: 0.4005\n",
      "Epoch 36/50\n",
      " - 0s - loss: 0.2510 - val_loss: 0.3815\n",
      "Epoch 37/50\n",
      " - 0s - loss: 0.2434 - val_loss: 0.3869\n",
      "Epoch 38/50\n",
      " - 0s - loss: 0.2410 - val_loss: 0.3788\n",
      "Epoch 39/50\n",
      " - 0s - loss: 0.2241 - val_loss: 0.3587\n",
      "Epoch 40/50\n",
      " - 0s - loss: 0.2115 - val_loss: 0.3668\n",
      "Epoch 41/50\n",
      " - 0s - loss: 0.2183 - val_loss: 0.3467\n",
      "Epoch 42/50\n",
      " - 0s - loss: 0.2089 - val_loss: 0.3455\n",
      "Epoch 43/50\n",
      " - 0s - loss: 0.2113 - val_loss: 0.3729\n",
      "Epoch 44/50\n",
      " - 0s - loss: 0.2032 - val_loss: 0.3505\n",
      "Epoch 45/50\n",
      " - 0s - loss: 0.1907 - val_loss: 0.3709\n",
      "Epoch 46/50\n",
      " - 0s - loss: 0.1916 - val_loss: 0.3365\n",
      "Epoch 47/50\n",
      " - 0s - loss: 0.1904 - val_loss: 0.3383\n",
      "Epoch 48/50\n",
      " - 0s - loss: 0.1800 - val_loss: 0.3299\n",
      "Epoch 49/50\n",
      " - 0s - loss: 0.1700 - val_loss: 0.3378\n",
      "Epoch 50/50\n",
      " - 0s - loss: 0.1663 - val_loss: 0.3331\n"
     ]
    }
   ],
   "source": [
    "x_tr, y_tr = x[train,:], y_raw[train]\n",
    "x_te, y_te = x[test,:], y_raw[test]\n",
    "dinp, epochs = (6,50)\n",
    "model = Sequential()\n",
    "layer_0 = Dense(units =input_d//dinp, input_dim = input_d,\n",
    "                kernel_initializer = \"random_normal\", activation = \"relu\")\n",
    "model.add(layer_0)\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "layer_out = Dense(units = 1,kernel_initializer = \"random_normal\", \n",
    "                  activation = \"linear\")\n",
    "model.add(layer_out)\n",
    "model.compile(loss=\"mse\",optimizer = \"adam\")\n",
    "train_history = model.fit(x = x_train, y = y_train, \n",
    "                          validation_split = 0.2, epochs =epochs, verbose = 2)\n",
    "plt.plot(train_history.history[\"loss\"])\n",
    "plt.plot(train_history.history[\"val_loss\"])\n",
    "plt.title(\"Loss Graph\")\n",
    "plt.legend(['loss', 'val_loss'], loc=\"upper left\")\n",
    "\n",
    "# #     model_test\n",
    "pre = model.predict_classes(x_test)\n",
    "acc = round(accuracy_score(y_test, pre)*100,2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
